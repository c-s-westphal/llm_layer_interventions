# Configuration for Gemma-Scope-2b SAE intervention experiments

# Model configuration
model_name: "google/gemma-2-2b"  # Gemma 2 series (d_model=2304)
sae_release: "gemma-scope-2b-pt-res-canonical"
sae_id_template: "layer_{layer}/width_16k/canonical"  # Gemma-Scope format
hook: "resid_pre"
layers: "0-25"  # Gemma-2b has 26 layers (0-25)

# Intervention parameters
alpha: 2.0
alpha_sweep: [0, 0.5, 1, 2, 4]
live_percentile: 90

# Corpus configuration
corpus_name: "wikitext2"
max_passages: 200
test_passages: 1500
max_len: 256

# Position sampling (10% for Gemma due to more layers)
position_fraction: 0.10

# Feature selection
feature_source: "csv"
neuronpedia_token: null
top_k: 3

# Computation settings
device: "auto"
batch_size: 16
seed: 1234

# Output configuration
output_dir: "outputs_gemma"
save_snippets: true
snippet_window: 20
num_snippet_examples: 5
